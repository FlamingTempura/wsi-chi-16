\documentclass{sigchi}
%\documentclass{sig-alternate-05-2015}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.


%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
%\toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)


% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy 

%\pagenumbering{arabic}

% Load basic packages
\usepackage[table]{xcolor}
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead 
%\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage[pdftex]{hyperref}
% \usepackage{url}      % llt: nicely formatted URLs
\usepackage{color}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{ccicons}
\usepackage{todonotes}
% \usepackage{natbib}
\usepackage{array}

%\usepackage{enumitem}
%\setlist{nosep}
%\raggedbottom

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={SIGCHI Conference Proceedings Format},
  pdfauthor={LaTeX},
  pdfkeywords={SIGCHI, proceedings, archival format},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.

\emergencystretch=\maxdimen % make it wrap properly

\begin{document}

\title{The Quantified Patient in the Doctor's Office: \\[0.1cm]Challenges \& Opportunities}

\numberofauthors{2}
\author{%
  \alignauthor{Peter West, Richard Giordano\\
    \affaddr{Faculty of Health Sciences}\\
    \affaddr{University of Southampton, UK}\\
    \email{\{p.west,r.giordano\}@soton.ac.uk}}\\
  \alignauthor{Max Van Kleek, Nigel Shadbolt\\
    \affaddr{Department of Computer Science}\\
    \affaddr{University of Oxford, UK}\\
    \email{\{max.van.kleek,nigel.shadbolt\}@cs.ox.ac.uk}}\\
}

% ACKNOWLEDGE: both wsi grant and EPSRC phd studentship

%\CopyrightYear{---} 
%\setcopyright{---}
%\conferenceinfo{---}{---}
%\isbn{---}\acmPrice{---}
%\doi{---}

\maketitle

% In practice, however, introducing such data into any clinical process in a way that they can be used effectively, and without presenting additional risk, could pose significant challenges that HCI as a field, may be able to help. 

\begin{abstract}
While the \emph{Quantified Self} and \emph{personal informatics} fields have focused on the individual's use of self-logged data about themselves, the same kinds of data could, in theory, be used to improve diagnosis and care planning.  In this paper, we seek to understand both the opportunities and bottlenecks in the use of self-logged data for differential diagnosis and care planning during patient visits to both primary and secondary care. We first conducted a literature review to identify potential factors influencing the use of self-logged data in clinical settings.  This informed the design of our experiment, in which we applied a vignette-based role-play approach with general practitioners and hospital specialists in the US and UK, to elicit reflections on and insights about using patient self-logged data.  Our analysis reveals multiple opportunities for the use of self-logged data in the differential diagnosis workflow, identifying capture, representational, and interpretational challenges that are potentially preventing self-logged data from being effectively interpreted and applied by clinicians to derive a patient's prognosis and plan of care.
\end{abstract}

\keywords{Quantified self; clinical decision making; self-tracking}

%\category{H.5.m.}{Information Interfaces and Presentation
%  (e.g. HCI)}{Miscellaneous} \category{See
%  \url{http://acm.org/about/class/1998/} for the full list of ACM
%  classifiers. This section is required.}{}{}

\category{H.5.2}{Information interfaces and presentation (e.g., HCI)}{User-centered design}.
\category{J.3}{Life and medical sciences}{Health}

\section{Introduction}

Empowering patients to ``take charge'' of their health is an idea frequently championed by politicians~\cite{kelsey_2015, Fox2015}, technologists~\cite{ihealth_2015}, journalists~\cite{goetz_its_2010} and healthcare experts alike~\cite{Swan2012}.  Yet, despite both government and industry-led initiatives across both Europe and North America to encourage this ``patient-led healthcare revolution,'' widespread adoption has been slow~\cite{vezyridis_adoption_2015}. 

One area, however, where individuals have been taking the lead in understanding their own health is the \emph{Quantified Self} movement. This primarily comprises of non-expert ordinary people who use technological tools to record and interrogate the minutiae of their physical and mental states over time~\cite{Swan2012}.   As the population of those interested in self-logging has grown, industry has responded with a vast collection of wearable and embeddable sensors which enable people to keep an accurate record of their health with low effort and high fidelity. For these reasons, it has been proposed that the Quantified Self movement can contribute to health care, helping clinicians diagnose and treat illnesses~\cite{Swan2009}. However, some clinicians outright reject the use of self-logged data, citing concerns about data quality, time constraints, and insufficient resources~\cite{sullivan_guess_2014}. % while the direct application of such sensors to understanding patients' particular symptoms, situations and lifestyles in the healthcare context would seem straightforward, , and will continue to improve by becoming less invasive, more comfortable, and more accurate.. Even when this information is provided by patients and is reference, it is still rarely, if ever, used for differential diagnosis.  

% little time for, or interest in, using wellness data collected by wearable devices. They don’t want to spend money on additional (and unproven clinical systems), and most of all, they don’t want to worry about keeping the data private.

What are the barriers to the use of self-logged data in critical clinical decision making settings?  This is a delicate question to approach for several reasons: first, during the course of a single patient visit, there are many kinds of decisions made by a different clinicians in different roles in different settings.   Paramedics in an ambulance, triage nurses within an emergency room, specialists in acute care units or hospital wards, to general practitioners (GPs) in their offices, all make decisions regarding patients under distinct situational and informational constraints~\cite{Croskerry2013}. Second, even if focus is centred around a single setting by a single class of medical professionals, such as GPs, there may be significant differences in day-to-day work practices between individuals. For example, the degree to which GPs use electronic medical records (EMRs) to organise patient data, how particular tests or treatments are prescribed, and the mechanisms that they use to maintain good patient relationships can vary~\cite{smellie_is_2002}. 

We focused on two clinical roles: primary care physicians on the ``frontline'' of the medical service, and secondary care specialists who work in hospitals.  Filling a gap in empirical research, we sought to understand the nature of evaluation and use of broadly-classed 'self-logged' patient data, particularly of the most common types facilitated by consumer health monitoring tools.  We wished to identify factors behind the underuse of self-logged data by clinical professionals, including \emph{what} was captured, \emph{how} they were  captured,  the \emph{representation} made available during a patient consultation, and other, yet unidentified, issues.  From this, we wished to extrapolate how such problems might be addressed through HCI research, such as by re-thinking tools people use to monitor themselves, the kinds of data they capture, or the ways that physicians and medical professionals might access and use self-logged data.

%\section{Overview of Approach}

%We began with an initial set of broad dimensions (Figure \ref{fig:qs}) chosen to represent the potentially many kinds of reasons that self-logged, QS data remains far from fulfilling its potential in clinical practice.

%\begin{figure}[tbpb]
%\begin{enumerate}
%    \item \emph{Data subject}: Lack of relevance or utility of \emph{what} is captured
%    \item \emph{Sampling method}: Problems with \emph{how} and \emph{when} information is captured
%    \item \emph{Accuracy}: Lack of trust in accuracy of instruments or capture method
%    \item \emph{Access}: Problems with physician \emph{access} to the data
%    \item \emph{Patient communication}: Interference with patient clinician communication
%    \item \emph{Workflow}: Interference or exclusion from diagnostic workflow
%    \item \emph{Biases}: Danger of cognitive biases introduced by data?
%    \item \emph{Other}: External factors? (\emph{data handling}, \emph{legal}, \emph{billing} etc)
%\end{enumerate}
%\caption{Initial dimensions for categorising barriers to clinical use of self-logged/QS data.}
%\label{fig:qs}
%\end{figure}

Since these dimensions encompassed a broad set of possible factors, including data-oriented problems (pertaining to subject, quality and sampling), situational constraints, and practice constraints, we wished to understand which, if any, of these dimensions had support from previous studies.  This led us to conduct a broad survey of medical literature, from which we identified a set of themes. This was followed by an empirical investigation from which we compared our findings to the identified themes and drew up a number of design implications for self-logging tools.  Finally, we discussed the limitations of our study and future work.

% To undestand this method we conducted a two phase invesitgation. Fi


% This paper presents a collective summary of our initial investigation in 

%Second, medical decision-making is making  we wish to approach this question carefully, considering What are doctors, nurses, specialists and medical professionals' views on the QS movement and self-logged data?  Is it a problem of what is being captured, how it is being captured, how it is represented, or presented?  In this paper, we summarise an initial exploration of these questions comprised of two stages: first, a literature survey pre-study, which informed the design of a set of role-playing probes with clinical specialists.


\section{Background} 

We contextualised our investigation against two closely related fields: the first, \emph{evidence based medicine}, seeks to apply empirical methods (such as used in epidemiology) for evaluating and improving the effectiveness of clinical practice and clinical decisions.  The second, \emph{clinical decision making}, examines the cognitive, interactional, and situational processes which influence how practitioners arrive at decisions under the practical constraints necessary for conducting their practices.  In this section, we introduce a high level view of how technology-enabled patient data-driven healthcare might look in the future and discuss the roles of evidence-based medicine and clinical decision making in our investigation.

\subsection{Visions for Data-Driven Healthcare} % / mHealth}
Notions of ``big data'' and ``data driven healthcare'' have inspired popular scenarios  of data-informed healthcare designed for the individual or stratified for groups of individuals. In the popular press, Thomas Goetz's \emph{The Decision Tree} outlined a vision in which every person will be DNA-tested at birth, and tracked with sensors throughout their lives~\cite{goetz_decision_2011}. The resulting data would be used to classify and compute optimal treatments and actions to support personalised medical treatments. Policy makers have set out national agendas towards such a goal. For example, the UK's Personalised Health and Care 2020 framework set out a vision in which health and well-being data, sensed from wearable and environmental sensors, would seamlessly integrate with patient health records by 2018~\cite{Personalised2014}.  The framework proposes that these data would ``fill in the gaps'' between visits with their GP or specialist, enabling clinicians to perform more personalised differential diagnoses at point of care.  It is anticipated that the introduction of such technologies will enable early onset detection of chronic conditions so that they can be controlled at their early stages, increase the quality of life for patients, reduce morbidity, and decrease national health-related costs~\cite{Swan2009}.   Initial enthusiasm for this vision is also evident in the US, with the US Food and Drug Administration's approving  consumer tracking devices for clinical trials, citing the importance of quantifiable analysis of physical activity to physiological monitoring~\cite{U.S.FoodandDrugAdministration2014}. % Exploratory data mining of sensed data is also likely to radically improve our understanding of both prominent and rare medical conditions, by revealing relationships among genetic, lifestyle, and environmental factors that might contribute to patients' symptoms. 

% quantified self vs self tracking vs lifelog vs prescribed sensors vs personal health records

% dont have much basis of saying this

Responding to this interest, the race to attract end-user consumers to better and more accurate physiological sensors has taken off over past eight years. New enterprises such as Withings, Fitbit and Jawbone, have been joined by veteran sport monitoring device makers such as Garmin, while established brands for sport to consumer electronics, such Nike and Sony, have developed attractive wearable tracking devices.  While still a minority, several enterprises, such as, iHealth and Withings, have had their consumer-level sensors certified for clinical use under the US FDA Class I and II medical device classifications~\cite{timeline_2013}. 

%\section{Barriers to Adoption}

% Yet despite this industry, political and tech-driven enthusiasm for use of self-logged QS data, clinical uptake has been slow.  For example, even with Apple's partnership with US healthcare providers around the targeted of their HealthKit framework have focused primarily on its use in telemonitoring for randomised controlled trials (RCT) rather than for differential diagnosis.  But even here, problems have been met with some partners pulling out, citing insufficient interest by both patients and physicians~\cite{}.  

% An informal survey by sullivan_guess_2014 found that clinicians were largely ''simply not interested in FitBit data'', assigning blame on several factors~\cite{sullivan_guess_2014}. The first was time; doctors cited simply not having the time to look at data brought in by patients.  The second pertained ot access; electronic medical records simply did not admit or integrate patient-supplied data.  Third were concerns about data quality, e.g. that devices available today were simply not yet accurate enough for clinical use.  Finally, doctors expressed legal concerns around use of the data; citing the potential to be held liable for data captured by wearable health sensors.

\subsection{Evidence Based Medicine}%: Clinical Decision-Making With Data Under Conditions of  Uncertainty} 

The term \emph{evidence-based medicine} refers to the use of epidemiological methods in both patient-level decision making and formulation of population-level clinical care guidelines.  In the first use of the term, David M. Eddy pointed out that while outcome analysis may make it seem that there is a clear path towards achieving more accurate, or at least consistent, diagnoses, things are never that simple in practice:

\begin{quote}
  Decisions might be variable but they are not whimsical or flippant. The variability occurs because physicians must make decisions about phenomenally complex problems, under very difficult circumstances, with very little support. They are in the impossible position of not knowing the outcomes of different actions, but having to act anyway.~\cite{eddy1990challenge}
\end{quote}

Eddy, thus, argues that, due to the complexities of assessing individual patients' conditions under the constraints in place, care targets and practice recommendations alone will not achieve better health outcomes.  Instead, working under extreme uncertainty is a necessity and should be embraced with the acknowledgement of the effects of various decision-making constraints and biases that are known to exist.

\subsection{Clinical Decision-Making} 

% TODO TODO 
 Modern texts for nursing and clinical evaluation have formalised the diagnostic process of determining a patient's disease or condition based on available evidence in the form of \emph{differential diagnosis}~\cite{thompson2002clinical}.  Briefly, this is usually described as the following procedure: first, the physician gathers all available information about the patient, creating a list of symptoms.  Then, the physician lists plausible candidate causes for the symptoms, prioritising the most urgently dangerous.  Finally, plausible causes are ruled out through tests or further observations, and treated systematically.  

%\subsection{Cognitive Dispositions to Respond}

%Recent research has revealed that clinical decision-making is often strongly influenced by cognitive bias, leading to preventable adverse events, worsened patient outcomes, and higher mortality.  Croskerry et al.  ~\cite{Croskerry2013}~\cite{Graber2002} argues that cognitive bias is a major cause of preventable diagnostic error, but is extremely challenging to study and reduce. This complements earlier work by~\cite{Kahneman1982}, in which people tend to use biases when making decisions under uncertainty, occasionally leading to severe errors. Many forms of cognitive bias have been identified -- Wikipedia lists over 350\footnote{Wikipedia -- List of cognitive biases -- \url{http://en.wikipedia.org/wiki/List_of_cognitive_biases} [Accessed 4th Sep 2014]} -- however, the consequences of biases in clinical decision making remain largely unexplored~\cite{Croskerry2013}. It is thus unknown how the provision of patient data may affect bias in clinical decisions.

% research questions
% By conducting a literature review of research in cognitive bias over the last 50 years, this dissertation presents a list of 11 cognitive biases which may affect clinical decisions made when patient data is used. From this, it has been identified that supplementing patient data may introduce further bias due to methods clinicians use to interpret patient data. Furthermore, it has been found that these biases usually have a more significant effect when decisions are made in acute scenarios, such as emergency rooms, where decisions must be made quickly with limited resources.

% definitions
%For the purpose of this paper, decisions which involve patient data are called \textit{evidence-based clinical decisions}. The term \textit{patient data} refers specifically to personal information which is collected by mobile apps and consumer devices, such as the number of steps taken, video lifelogs, location history and status updates on social networks. Patient data is subsequently \textit{interpreted} by clinicians, which refers to the observation, analysis and sense-making of data.


% stuff from pete's masters thesis

% a little bit about all the potential ways QS could help

%~\cite{Swan2009} has called for use of this data as a means to supplement healthcare, which would increase ``information flow, transparency, customization, collaboration and patient choice and responsibility taking, as well as quantitative, predictive and preventive aspects.''. The use of self-tracking data in healthcare is not new. The health diary has been popular since the 1950's as a data collection method~\cite{Richardson1994}. ~\cite{Richardson1994} note that particular types of data that are recorded include pain, fatigue, medication use and dietary intake. This shares overlap with health and well-being data, suggesting there are existing use cases where health and well-being data could be supplemented.~\cite{OLoughlin2013} has demonstrated the use of lifelogging for the purpose of increasing diet awareness. However,~\cite{Swan2009} has discussed that there is currently little adoption, perhaps due to the barriers of technology in clinical environments.


\section{Pre-study: Literature Review} 

 The objective of our literature review was to establish a framework from which we could then identify areas of self-logging in clinical settings that require further investigation, as well as opportunities for HCI research to help.  

\subsection{Literature Review: Method}

We started with a set of search terms broad enough to encompass studies of clinical practice where patient-logged data (both paper-based and digital, manual and automatic) were introduced into a clinical setting.  To do this we searched PubMed, Google Scholar and the ACM DL for keywords ``patient diaries'', ``care diaries'', ``well-being diaries'', ``self-report diaries'', ``quantified-self'', ``self-tracking'', ``self-logging'', ``smartphone apps'', and ``wearable sensors''.

Since we wanted to focus on the usage of data by medical experts, we excluded studies about use by patients themselves, such as for feedback, reflection, goal setting and self monitoring, including behaviour-change studies and studies of motivation to self-diaries, which were prevalent in the HCI community.   Focusing only on existing practice, we omitted papers describing new interfaces and systems that have not had substantial adoption.  We also excluded papers discussing the capture side of health diarising and life-logging by patients, except where aspects of capture affected its later use.  We were careful to include papers that discussed any issues relating to the use of patient data in clinical setting, including those that discussed human factors issues specifically, to more broadly operating rooms and emergency rooms.

We then broadened our search to include studies that discussed the use of patient data in medical decision-making, including both patient-supplied and clinical data held by providers themselves.  We included ``telemonitoring'', and ``electronic patient records''.  This  hoped to find a broad range of factors spanning human-factors issues to social, cultural, institutional, situational, among others.

For each paper, we identified factors that hinder the clinical use of data, which were first added to a spreadsheet and linked to their original source.  After examining each of the papers, two researchers organised the list into themes, attempting to merge all problems with the same underlying cause, while keeping those that did not overlap distinct.  %During this process, we identified a set of themes related specifically to cognitive biases, which we analysed and report separately below.

\subsection{Literature Review: Results}

\begin{table*}[th]
    \def \w {\cellcolor{blue!40!black}\textcolor{blue!40!black}{X}}
    \setlength\extrarowheight{0.02cm}
    \setlength\tabcolsep{0.048cm}
    %\def\arraystretch{0}
    \centering
    \small
    \begin{tabular}{>{\raggedright}p{4.3cm} !{\vrule width 1.3pt}
        p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} !{\vrule width 1.3pt}
        p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm} | p{0.5cm}}
        
    %\hline
    & \multicolumn{14}{>{\centering}p{8.42cm} !{\vrule width 1.3pt}}{\textbf{Self-logged data}}
    & \multicolumn{8}{>{\centering}p{4.7cm}}{\textbf{Patient data}} \\ \hline
    
    % use of self-recorded data by clinicians
    & \centering{\cite{chung_more_2015}}
    & \centering{\cite{ancker_invisible_2015}}
    & \centering{\cite{becker_mhealth_2014}}
    & \centering{\cite{sullivan_guess_2014}}
    & \centering{\cite{almalki_use_2015}}
    & \centering{\cite{patel_probing_2012}}
    & \centering{\cite{Swan2009}}
    & \centering{\cite{millar_shared_2004}}
    & \centering{\cite{ancker_you_2015}}
    % use of IT by clinicians
    & \centering{\cite{alsos_mobile_2012}}
    & \centering{\cite{frankel_effects_2005}}
    & \centering{\cite{zeldes_information_2011}}
    & \centering{\cite{chen_unpacking_2011}}
    & \centering{\cite{patterson_human_2004}}
    & \centering{\cite{schoenberg_weaving_2000}}
    & \centering{\cite{vikkelso_subtle_2005}}
    & \centering{\cite{hughes_2.0_2008}}
    % reliability/accuracy of self-recorded data
    & \centering{\cite{demonceau_contribution_2015}}
    & \centering{\cite{tsoukalas_data_2015}}
    & \centering{\cite{frier_hypoglycaemia_2015}}
    & \centering{\cite{khariwala_self-reported_2015}}
    &~\cite{dontje_measuring_2015} \\[0.05cm] \hline
    
    \textbf{Data capture} &&&&&&&&&&&&&&&&&&&&&& \\ \hline 
    Relevance
        &\w &\w &   &   &   &\w &   &   &   &   &   &   &   &   &   &   &\w &   &\w &   &   &   \\ \hline 
    Quality
        &   &   &   &\w &   &\w &   &   &\w &\w &\w &   &   &\w &   &   &   &   &\w &   &   &\w \\ \hline 
    Completeness
        &   &\w &\w &   &   &\w &   &   &\w &   &   &\w &\w &   &   &   &   &   &\w &   &\w &   \\ \hline

    \textbf{Data access} &&&&&&&&&&&&&&&&&&&&&& \\ \hline 
    Selective disclosure
        &\w &\w &   &   &   &   &   &\w &   &   &   &   &\w &   &   &   &   &   &   &   &   &   \\ \hline 
    Representation
        &\w &\w &\w &   &   &   &   &   &   &   &\w &   &   &   &   &   &   &   &   &   &   &   \\ \hline 
    Interoperability
        &\w &\w &\w &   &   &   &   &\w &   &   &\w &   &   &   &\w &   &   &\w &   &\w &   &   \\ \hline
        %\midrule
    \textbf{Clinical practice} &&&&&&&&&&&&&&&&&&&&&& \\ \hline
    %Interpretation
    %    &   &   &   &   &   &   &   &   &   &   &   &   &\w &\w &   &   &   &   &   &   &   &   \\ \hline 
    Data literacy
        &\w &   &\w &   &\w &   &\w &   &   &   &   &   &   &   &   &   &   &   &\w &   &\w &\w \\ \hline 
    Doctor-patient relationship
        &\w &   &   &   &   &   &   &\w &\w &   &   &   &   &   &\w &\w &   &\w &\w &   &\w &   \\ \hline 
    %Professional autonomy
    %    &   &   &   &   &   &   &   &\w &   &   &   &   &   &   &   &   &   &   &   &   &\w &   \\ \hline 
    Legal issues
        &   &   &\w &\w &   &   &\w &   &   &   &   &   &   &   &   &   &   &   &   &   &\w &   \\ \hline
        %\midrule
    \textbf{Situational constraints} &&&&&&&&&&&&&&&&&&&&&& \\ \hline
    Time
        &\w &   &   &   &   &   &   &   &   &   &   &   &   &   &   &\w &\w &   &\w &   &\w &   \\ \hline
    Information overload
        &   &   &\w &\w &   &   &   &   &   &   &   &   &   &   &   &   &\w &   &   &   &   &   \\% \hline
    %Effort
    %    &\w &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   \\
    
    %\hline
    \end{tabular}
    \caption{Major themes identified within the literature review, separating papers by whether they discussed self-logged data, or patient data such as telemonitoring or EMRs.}
    \label{tbl:themes}
\end{table*}

From an initial set of 2340 results, we identified 429 papers that contained at least one of the search terms among keywords and the abstract. We then identified 22 relevant papers according to the criteria defined above. This set allowed us to derive the 11 themes contained in Table~\ref{tbl:themes}. We these these into four categories: data capture, data access, clinical practice and situational constraints. %, omitting themes pertaining to cognitive dispositions to respond which we discuss next.

\subsubsection{Data capture}

This category pertains to how individuals conduct self-logging, and the devices used for doing so. Three themes were identified in this category:

\begin{itemize}
    \item \textit{Relevance}. Individuals may decide for themselves what information is important to self-log. This decision is influenced by the individual's own knowledge of their condition. Without guidance from clinicians, the types of self-logged information presented by a patient may not align with what the clinician sees as useful and actionable.
    %data may be irrelevant, or useless within the context potential barriers to adoption of self-recorded data within clinical practice, derived from literature of clinicians use of. Inapplicability IT/mHealth/telemonitoring/patient-recorded data. 
    
    \item \textit{Quality}. Most consumer devices for self-logging are not approved for medical use. Clinicians often perceive the data quality and sampling from such devices to be poor and perceive the devices to be unreliable, and therefore unwilling to use them. Studies have however demonstrated some devices to have good reliability for particular purposes.
    
    \item \textit{Completeness}. Individuals who self log often do so sporadically, leaving out readings which may be important. Individuals attributed this to cumbersome logging tools that take time and effort. Such spotty data is suspect by clinicians who look for detailed information patterns and anomalies in data.
    %data incomplete, also, is the patient able to provide needed information (e.g. their experiences during an episode)?
\end{itemize}

\subsubsection{Data access}

This category pertains to factors contributing to how individuals access and use self-logged data. Three themes were identified in this category:

\begin{itemize}
    \item \textit{Selective disclosure}. In addition to not disclosing data or information because they patient thought them to be irrelevant, some patients raised privacy concerns with sharing information with health providers because they felt no control over how their data might be used outside the clinical setting. Some patients therefore concealed certain information from their health providers.
    
    \item \textit{Representation}. Health providers use standardised forms for certain types of data, including blood pressure, symptom history and glucose level. However, due to the quantity and variety of self-logging apps and devices, there is a lack of standardisation of how self-logged data is represented.  Exercise logs, for example, often have distinct data structures and different meanings of `activity level'. (Furthermore, applying standards to data may remove important parts.)  This heterogeneity in data representations results in ambiguity, and adds difficulty to interpreting self-logged data.
    %flaggin up of things which are not noteworthy. A lack of flexibility in how information is presented .
    
    \item \textit{Interoperability}. A lack of standardisation of how data is represented creates problems when trying to share between colleagues and other health providers. There are concerns that self-logging tools will be poorly integrated into existing health provider tools. In particular, there are challenges regarding the integration of Web-based services (which many self-logged devices and apps are) into legacy systems. Many self-logging apps do not provide a means of exporting data, or they only do so in propriety APIs which are not intuitive for clinicians to use.
\end{itemize}

\subsubsection{Clinical practice}

This category pertains to how clinicians' training affects how effectively self-logging data can be used, and the potential barriers to introducing self-logged data to clinical practice. Three themes were identified in this category:

\begin{itemize}
    %\item \textit{Interpretation} - one physician may use data in a different way to another
    
    \item \textit{Data literacy}. Clinicians are concerned that they do not have appropriate expertise or training to effectively use or validate self-logged data. Contributing factors include the lack of standards for data representations, the lack of access to appropriate electronic tools for analysis, not being familiar with new tools for self-quantisation, and the wide variety of data. Clinicians may rely on specialists to interpret the information, but they may not always be available when they are needed.
    
    \item \textit{Doctor-patient relationship}. The use of mobile devices in a clinical setting can draw the physician's attention attention away from the patient and to the device. User interface and physical form factor affect how doctors and patients conduct face-to-face discussion and nonverbal communication. These aspects are crucial because they allow clinicians to better understand the patient's condition, and contribute to patient satisfaction. Exam rooms are often laid out such that clinicians can quickly switch between looking at a computer and the patient, but poor user interface can cause clinicians to spend much longer looking at the computer and disengaging from discussion with the patient. Some clinicians are concerned that patient self-logging may threaten professional autonomy, with concerns that patients may begin to make decisions which would be better made with clinical advice.

    %takes time away from patient. e.g. the patient get's upset if the QS is disregarded. in modern gp practices, EMR screen is next to patient, so doctor can switch between patient and EMR quickly - incompatible with planned out environment.  Patient talks to clinician, clinician is trying to use computer so doesn't listen 
    
    %\item \texti{Professional autonomy} - "Professional territorialism and perceived threat to professional autonomy and/or scope of practice" [32] harmful to patient relationship: patient self-diagnosis
    
    \item \textit{Legal issues}. Clinicians raised concerns that the lack of regulation on self-logging apps and devices has implications for safety, security, data protection, and reliability of using their associated data. Health providers have also raised the issue of legal liability, particularly if data fell into the wrong hands and exposed them to privacy violations, or if decisions were made based on poor quality data.
\end{itemize}

\subsubsection{Situational constraints}

This category pertains to constraints that exist within healthcare situations. Two themes were identified in this category:

\begin{itemize}
    \item \textit{Time}. Clinicians often need to work within tight time constraints, within which they are already under pressure. Clinicians have been sceptical about how effectively they could use self-logged data within these time constraints. Some said that they could be more effective when working between visits, but that work is not recognised in those times, so are discouraged from doing so. Cumbersome user interfaces are frustrating to clinicians because of the time it takes to complete a task or to find a relevant piece of information. Moreover, clinicians are expressed concern on the time it takes to document how such data is used, even when it is irrelevant to the current situation.
    
    \item \textit{Information overload}. The sheer quantity of information generated by self-logged tools is seen by some to be a liability, reducing  productivity, increasing levels of stress, and reducing morale. The way in which information is presented contributes to the extent to which overload affects a user.  That is, the simpler the display of complex information, the better.
    
    %\item \textit{Effort}. perceived usefulness. belief that it won't be worth it - is this a rationale belief? not valuable
\end{itemize}

%\subsection{Literature Review: Conclusion}

%This literature review has confirmed that a wide variety of factors may contribute to how self-tracked data is used by clinicians. By categorising these themes, we suggest form into a chain of dependencies and events. First, data capture is facilitated by patients using self-tracking tools. The reliability of the tool and how the patient uses it affect how useful a clinician will find that information. Second, patients provide the information to the clinician, perhaps mediated by the self-tracking tool, and clinicians attempt to access and use the data. Third, the clinicians make sense of and use this information according to their training and practice and within tight time constraints.

%introduce Markov model?

% We anticipated that the slow uptake of the use of self-logged QS could potentially be attributed to a large number of factors, and thus decided it prudent to first do a survey of medical, quantified-self and HCI literature.  Our objective  was to establish a theoretical framework from which we could then identify areas that could require further investigation, as well as opportunities where research in HCI could potentially help.  

%% party here

% \subsubsection{Cognitive Dispositions to Respond}

% During the first phase, we encountered a substantial body of work pertaining to  situational and psychological factors that influenced the decision-making performance of medical experts in clinical settings.  Croskerry's \emph{Cognitive Dispositions to Respond}~\cite{} 

% We started with MEDLINE, Web of Science, and the ACM DL
% Using the keywords "medical decision-making", "clinical decision-making", "patient-logged data", "quantified self", "data in hospital admission", "GP visit", searches were performed on BMJ, MEDLINE, CINAHL, Google Scholar, Web of Science, Springer, and the ACM DL.  This collectively resulted in 2982 results, many of which
% d
% Temporal constraints
% Situational constraints
% UI/UX constraints 
% Workflow constraints
% Cognitive Biases
% Training/background constraints
% Exclusion criteria:
%   Anything that only involved the patient, including behaviour change studies


%   UK Prime Minister Gordon Brown,  pronounced in 2008 that more effective ``future will be one of patient power, patients engaged and taking control over their own health and healthcare.'' – Gordon Brown, U.K. Prime Minister


\section{Methodology}

\subsection{Overview and logic}
We conducted role-play interviews with clinicians from primary and secondary care within the United Kingdom and United States. The purpose of this study was to elicit reflections on and insights about using patient self-logged data in two distinct written vignettes based on real medical cases.  Vignettes are frequently used to examine clinical judgements and decision-making among health professionals~\cite{evans2015vignette,redelmeier1995probability} because these minimise confounding variables that may be introduced by using real patients or actors. A clinician's response to an examination of a patient might be unpredictably influenced by the patient’s physical appearance, ethnicity, body language, verbalisations, or eye contact. An actor would not respond believably to detailed questions from a clinician during a physical exam, and both actors and patients might change their responses between sessions. We also wanted clinicians to think out loud as they worked through a case, and this seemed neither realistic nor natural with a human subject.  The use of paper-based vignettes based on real medical cases therefore permitted us to focus on initial presentation (before a care record and charts were synthesised) and observe what further data or history a clinician would seek. Because we focused on forms of self-logged data which are not in widespread use in clinical practice, we were unable to find medical cases in which these forms of data were used, hence those data are drawn from other subjects. 


\subsection{Recruitment}
We recruited 10 participants (3 female) who were full time, highly experienced doctors.  Three participants were general practitioners (GPs) in England, practising in three different urban settings; seven were board-certified specialists (Sps) from a variety of specialisms (such as rheumatology, nephrology, cardiology, hepatology, practicing) at a single large urban hospital in the United States.  We used a small snowball technique to build our population of participants.   We recruited both specialist hospital doctors and general practitioners with the expectation that this would provide a variety of distinct approaches to selecting, judging, and using information and data.  The participants are listed in Table~\ref{tab:participants}.

\begin{table}[t]
    \centering
    \setlength\extrarowheight{0.0cm}
    \begin{tabular}{lcccc}
     \toprule
     ID & Level of care & Gender & Country & Speciality \\ \midrule
     GP1 & Primary & Male & UK & -- \\
     GP2 & Primary & Female & UK & -- \\
     GP3 & Primary & Male & UK & -- \\
     
     Sp1 & Secondary & Male & USA & Nephrology \\
     Sp2 & Secondary & Male & USA & Rheumatology \\
     Sp3 & Secondary & Male & USA & Pulmonology \\
     Sp4 & Secondary & Male & USA & Hepatology \\
     Sp5 & Secondary & Male & USA & Cardiology \\
     Sp6 & Secondary & Female & USA & Nephrology \\
     Sp7 & Secondary & Female & USA & Pulmonology \\
     \bottomrule
    \end{tabular}
    \caption{Level of care, gender and country of practice for participants, and speciality for participants in secondary care. Participants comprised of general practitioners (GP) and specialists (Sp).}
    \label{tab:participants}
\end{table}

\subsection{Data Collection}
We conducted semi-structured interviews using a protocol designed around the following areas of interest:
\begin{itemize}
    \item How would doctors judge patient-supplied data?
    \item Would doctors use patient-supplied data?
    \item How does patient-supplied data align with current work flows and work practices?
\end{itemize}

We drafted two written vignettes of patients presenting with a set of symptoms.  These were actual cases of real patients selected from set of vignettes derived from the ``Think Like a Doctor'' column in the New York \emph{Times}.  These were edited  so that they would be understandable to doctors in the US and UK, and added further information on the the nature of self-logged data provided by the patient.  The vignettes were written as narratives that outline the patient's symptoms, some background and history, and the reasons why the patient decided to see the doctor.  

In the first vignette, a middle aged man finds it difficult to sleep, feels nauseous, sweaty, out of breath, and finds, when lying in bed, that his legs disobey his mental orders to stop moving.   There is some background history including that he has a \emph{patent foramen ovale} (PFO, or a small hole in the tissue separating the right and left atria), suffered a mild stroke years ago, is on blood thinners, experiences occasional panic attacks where he feels his heart pounding in his chest.  We added that his wife convinced him to become a strict vegetarian, and she bought him a heart monitor that he regularly uses to check his pulse. When he visits the doctor, he brings a printed Excel chart of his resting pulse in the morning, afternoon, and late evening every day for the past month. Aside from a few spikes of 130bpm, his pulse hovers around 85bpm.  His pulse increased to about 100bpm in the three days before his doctor visit.  \emph{The New York Times} vignette does not include the patient's vegetarian diet or that he has a heart monitor that he uses daily; this is information we added.

In the second vignette, a university student is lightheaded, dizzy, her body feels heavy, she feels anxious, has blurred vision, has headaches, and she is prone to fainting if she tries to stand, and her lips are bluish. She had surgery on her back twice about 18 months ago: once to fix to fix a pinched nerve, and again to cut out an infection that rooted there.  She had been on antibiotics after the surgery, and the infection had cleared.  She does not smoke, and her only alcohol consumption was an occasional glass of wine.  She drinks copious amounts of coffee to keep her awake, especially during exam periods, and she was worried about this.  She installed an app on her iPhone that kept a record of her caffeine intake, and she used this to record her caffeine intake. She kept a log that she printed and brought with her when she visited her doctor. The log chart shows that she was consuming in excess of 1000mg of caffeine a day, until the day before she visited her doctor when she had no coffee.  \emph{The New York Times} vignette does not include this patient's caffeine intake, nor that she measures it; this is information we added.

None of the participants had seen these vignettes in advance, and none knew in advance the true diagnoses: the man had a vitamin B12 deficiency, and the student was diagnosed with postural tachycardia syndrome.  We chose these vignettes because they are relatively complicated, such patients could be seen by both GPs and specialists, a diagnosis is not obvious, and they would provoke a chain of thoughts and judgements by the participants.

We distributed a vignette and chart to each participant, and asked them to think out loud as they were reading them.  We asked for general comments on the case, and how they would use the information supplied by the patient.  GPs were interviewed individually; specialists read and commented on the vignettes one-by-one in a group.  To overcome group influence, we asked specialists to write down their thoughts on the printed vignettes, and to hand them in at the end of the session.  Interviews lasted for about an hour with specialists and 30 minutes with GPs, and were recorded and transcribed.

\subsection{Data Analysis}
The research team individually analysed each transcript, swapped analysed transcripts, and re-analysed them~\cite{pope2000qualitative}.  In the first iteration, we developed a set of keywords (such as 'Training,' 'Patient History,' 'Exam') that characterised what the participants were saying as they read through the vignetes and then discussed them, as well as what they had written on the printed vignettes that we distributed. At this stage, we were interested in only in identifying and tagging what they said; this was the qualitative equivalent of performing simple descriptive statistics on a dataset.  Using inductive qualitative methods,~\cite{corbin2014basics} we then iteratively developed a coding scheme related to participants' information and data acquisition practices, their judging and trusting data and information, the patterns of interrogating data and information. Subsequent analyses allowed us to discern how both data and information content and structure support or hinder their work practice and flow of thoughts.  Our final analysis helped us focus on the cyclic nature of their strategies, and helped us construct a model of this.


\section{Results}

\newcommand{\bigquote}[2]{
    \begin{quote}
        #1~--~\textit{#2}
    \end{quote}
}

In this section, we present results in the order as they unfolded during the clinicians' thinking out load as they read the vignette, the order of their notes on the printed vignettes, and in the followup discussion.
    
%\subsection{Reorganising information into a structured history}
%One of the first comments from the clinicians was the lack of specificity in the vignettes, and how the information looked to be both ambiguous and unstructured.  One Sp laughed, "Hole in the heart?" while another commented, "What exact meds is he on?"  Five hospital specialists (Sp1, Sp2, Sp3, Sp4, Sp5) stated that they needed to reorganise the vignette to form a structured medical case. In particular, they wanted a more complete patient history, test results, dates of tests, and a clear and unambiguous time line.  This would allow them to understand how the patient's current situation  fits within a time line, or whether it is a unique event. GP1 said he needed more specific information about the patient's medication, rather than just ``a couple of blood thinners''.

%\bigquote{Did it really start 18 months ago? Was that when it really started? Or has there been a pattern of it going on, and frequent doctor visits and nothing found. It's just anything more. Has she had recent blood work? Has she had recent x-rays or tests that might tell us that there's real pathology? Or that there's something to focus on.}{Sp3}

%\bigquote{I can't see the big picture.  It's all mixed up.  I need to reorder what I'm reading so I can see if there's a general diagnosis.  I can't see the big picture with the way this information is provided.}{Sp6} %[General agreement]

%\bigquote{You should understand that we'd never get a case report like this.  I'm looking for test results, meds, notes in order.  There are no lab forms here. I don't have anything to work with except for what I'm reading here.}{Sp4}

%\begin{quote}
    %``for me, there's an incomplete history here. So, the first place I would go is back to the patient, and try to pull out, what are the details of the timeline really, the timeline. Is this a distinct event or are this linked somehow, and pull it out from her, the history.'' Sp2
%\end{quote}


%Because there was no patient, some clinicians role-played the patient:

%\begin{quote}
%    ``She's a student, third year… probably coming up to exams. Is it a tension headache?'' (GP1)
%\end{quote}

%\begin{quote}
%    ``She's a student, I suppose I'm already thinking alcohol psychology.'' (GP2)
%\end{quote}

\subsection{Evaluating the data quality and completeness}

GP1 stated that he would need to understand how the patient recorded pulse rates. One important factor raised was the need to understand if any illness or medication had interacted which may contribute to a change in the patient's heart rate. GP2 and Sp2 mentioned they would need to understand what the patient was doing at the recorded times, particularly during the spikes, and Sp2 needed to know what other symptoms the patient was experiencing at those times. The heart rate chart provoked comments on the reliability of the data, relating to both the accuracy of the recording equipment and how the patient recorded the data. GP1 explained that he could not assume that the data was ``objective'', and that spot checking was not sufficient - he would instead need a trace using his own calibrated equipment, which he can trust:

\bigquote{I want to use my machine, which has been pre-calibrated, not off the shelf, because I don't know about this machine's calibration. Can I trust all the data? No. Can I assume all the data is correct? No. So I have to use a calibrated machine which I feel happy with, that I've used before, and which has had an electrician or someone who said this machine is accurate. And then I apply that test - a very quick non-invasive test. I can do a heart trace in my office to see if I can spot anything.}{GP1}

All physicians said that they could not really understand the significance of the data without having the patient in front of them.  In addition to conducting a physical examination, they would look for non-verbal and subtle cues.  For example, GP1 explained that he would observe how the patient behaves by walking with them from the waiting room to the office. 

\bigquote{I observe the way they sit, or stomping gait, or something. You're eyes just think oh! And then your connection will be made consciously and unconsciously.}{GP1}


\subsubsection{Understanding the patient's motivations}

The patient's motivations for self-logging were a factor in determining whether the data should be used. There was general agreement among all participants that it is important to know \emph{why} these patients took the time to record their pulse and caffeine intake. Pertaining to the caffeine chart, GP3 needed to understand why the patient recorded it, when it is not a normal thing to do:

\bigquote{I would try and ask a little bit more about this caffeine chart and why she's done this anyway, just to have an understanding of the reasons. Because not everyone charts their caffeine intake.}{GP3}

In particular, Sp3 suggested that presentation of a caffeine chart suggested underlying psychological issues, such as being overwhelmed and struggling with their studies, job or relationship. Referring to both vignettes, Sp5 asserted that the patient must be obsessed to record data, suggesting an underlying psychological issue. For Sp4, The mere existence of a heart rate plot provoked a jump to psychological issues:

\bigquote{They're faking it. If someone brought this chart to me, there's a red flag that this guy's got psych issues.}{Sp4}

Sp3 commented that, among his patients, it was not uncommon for engineers to bring plots of self-logged data to consultations, and Sp7 proposed that patients bringing in data will, in time, become normal practice. Nevertheless, the overarching tone of the discussion questioned the motives of patients who present self-collected data, or they believed that collecting such data might mask a larger medical or psychological issue. Sp6 said that a ``lot of patients come with a diagnosis that they put on'', with Sp1 following on from this by saying:

\bigquote{It's typical that patients like this that come in and they give you stuff, you get this whole story, and then they want you to focus on this it. It takes your attention away. Or they're going to tell you ``this is the reason why all of this is going on,'' and then you have to say ``well, OK, but let's just put that aside''.}{Sp1}

GP3 needed to understand why both patients had brought charts, expressing confusion at their presentation and questioning what they wanted out of their consultations.

\bigquote{If she's still feeling unwell, I would then refer back to the caffeine chart, especially if she's a student and knowledgeable about that. That might be cause to listen. I'd say, ``Hang on. OK, tell me more about this, and why? Why are you doing that?''}{GP3}

GP1 explained that they would question the presentation of self-collected data because in their experience "there is a lot of pressure for GPs to prescribe", and that it was necessary to resist this because patients push for prescriptions of "poorly evidence based things." 

\subsubsection{Deciding how to use the self-logged data}

Sp1 explained that a decision needs to be made on how much time they would need to spend using the data:

\bigquote{I think when you look at something like this, you do have to be able, in your own mind, to say, ``OK am I going to spend 30 seconds on this, or am I really going to spend more time and give it more importance?'' And I think that's what we face a lot.}{Sp1}

Sp1 explained that patient provided data sources will add layers of data assessment to practice, and questioned whether this information may adversely affect efficient work flow.

\bigquote{The layers of information, data assessment -- it's ramping up and up, and all of these devices are certainly adding, or will add, yet more of this. Certainly here, we get a lot of things faxed to us. We get to know patients who come here. And then they go somewhere else, but then we are getting their lab work over and over again - their X-rays, their visits. At some point you have to ask yourself, ``what is efficient here and what is not?''}{Sp1}

Sp7 concurred, explaining that this adds "complexity to an already complex medical interface."  Sp4 suggested that age of the clinician may be a large factor in acceptance of patient provided data arguing that medics do not usually change their practice significantly over time.

\bigquote{Younger doctors that are coming in are seeing patients for the first time and they're used to people bringing in this kind of stuff and will put more thought into it. Doctors are typically creatures of habit. You've been doing something for ten, fifteen years the same way, you're going to carry on doing it. There are minor changes that happen in terms of pathology and diagnostics, but in general you're used to doing it in a certain way... So I think it remains to be seen, but when I think what I'll be doing in 10 years time in terms of how I'll be managing a patient, it will be very similar.}{Sp4}

One GP raised the need for physical exam, and that the scenario and data alone were insufficient for making a diagnosis.

\bigquote{I haven't even touched him yet. After my history I'd do an examination, feeling the pulse myself, feeling if its strained, the quality, the character, and whether I can do anything to the pulse. And then listening to his heart and doing the various movements and motions -- sit forward sit back -- to see if he's got any murmurs associated with this. I'd see if he's been compliant with his anticoagulants -- maybe he is maybe he isn't.}{GP1}

%\subsection{Using (or ignoring) the patient-provided data}

There were differences in how useful the clinicians found the self-logged information. GP3 chose to ignore the caffeine, stating that it was irrelevant, but conceding that he may back to it later. Despite asserting that he would also ignore the heart rate plot, he did observe that there were high peaks which motivated questions about what the patient was doing at those times. 

\bigquote{But certainly, I don't think this chart would influence me. The only influence it would have is to try and understand more about why he did this and what he wants out of the consultation.}{GP3}

%Sp4 said the heart rate was "underwhelming," essentially showing normal heart rate with a couple of spikes. GP2 said that the information conveyed in the heart rate plot was not significant, with high rates explained by any activity the patient could have been doing at the time. GP2 said that blood pressure data would have been more useful to her than a pulse rate.

Sp4 stated that the presentation of this information would sway him toward caffeine as a cause:

\bigquote{I think if you see her at the office and she brings you this immediately you start thinking this is all from too much caffeine.}{Sp4}

GP1 said the caffeine chart suggested a coffee-withdrawal headache. He verbalised a unit conversion in trying to understand how much caffeine 400mg is.

\bigquote{Right! This could be a coffee headache. Well if you stop drinking coffee you get a headache. If you start drinking coffee you get a headache. Daily consumption -- wow - above 400mg, 150mg per cup. Yeah, so this could be a coffee withdrawal headache.}{GP1}

%GP1 went on to suggest that he might advise the patient to have a graded reduction in caffeine. Note in the quote above that GP1 was able to make a unit conversion of caffeine to cups of coffee, and was also able to note that this is an excessive amount. 
Sp2, Sp3 and Sp5 did not know what a normal caffeine level is, and could not comment on whether the headaches were potentially caused by caffeine withdrawal. Similarly, although Sp2 made an initial observation of the heart rate chart based on his own beliefs of how heart rate varies, he said he would need to consult a cardiologist to fully understand it:

\bigquote{Well one thing that struck me is how little variability there was in the heart rate during the time of the day. I would need to ask a cardiologist, but I thought there was greater variability in heart rate.}{Sp2}

\subsection{Prioritising patient safety}

In general, the clinicians would identify a set of possible conditions, and then array from the most serious to the least.  In addition, they would use information to construct a safe care pathway for the patient.  One specialist said,



%Another said,

%\bigquote{So I'll try and keep him safe, and then try and reduce his risk from this condition.}{GP1}

\bigquote{I'm really just looking for things that are more serious, like trying to see what's the worst possible thing that she could have, approaching it that way. And hopefully it will turn out to be nothing... I'm looking for what's the worst possible thing the person could have and work backwards from there...} {Sp5}

%\bigquote{I'm confident about what I've set in place is a safe plan, and I suppose that's the bit I need to be confident about... I think there's a plan, and I'm  confident that the plan is reasonable... I'll do an ECG, I'll do some blood tests, I'll see him again soon. He'll keep a note of what's going on. If something else happens in the meantime he'll call. So I think that's safe.}{GP2}


In ruling out the worst cases, GP1 said that knowledge of caffeine consumption changed his perspective on possible causes.  Although caffeine withdrawal initially seems likely, there could be a far more serious tumour. After reading the scenario and devising a plan, he said:

\bigquote{At the moment I've chopped, chopped, chopped, chopped, and we come to here. And now I think, ``Right, we've pruned off all of that, now I've got the bare tree.'' It's deciduous, not perennial! And it's very easy to see, this is my path now. It's your heart, mate. And I need to do just one or two tests to show. Otherwise the trunk of this tree becomes thicker, and I will go that way. That's how I think.}{GP1}


% \subsection{Summary of findings}

\section{Discussion}

Most of the major themes we identified in our literature review were found in our results; \emph{Relevance}, \emph{Quality}, \emph{Completeness}, \emph{Doctor-Patient Relationship}, \emph{Information Overload}, and \emph{Time} were especially prominent. Little related to \emph{Legal Issues} and \emph{Selective Disclosure} emerged. Our empirical work, however, suggests that have little to say to HCI research or practice. In this section we interpret an extended set of  themes resulting from our analysis into how future Quantified Self and self-logging tools might be made to better support use in clinical diagnosis. % these are incomplete and, moreover,

% = Intro - summarise findings, introduce themes=
% == Summarise relevant findings ==
%    lack of integration into workflow
% == Relate to literature ==
% Differences in how patient data is used
% Standardise it?

\subsection{Evidence Gathering in Support of Risk Mitigation} % (support & respect clinical workflow?)
\label{sec:riskmit}

Sets of interrelated activities: \emph{Discovery}: Gather information and evidence; \emph{Evaluation}: Evaluate the evidence for quality, reliability, validity, completeness; \emph{Form initial hypotheses}: Formulate a possible list of diagnoses based on the evidence presented; \emph{Identify knowledge gaps} that are needed to test hypotheses (rule out possible diagnoses) \emph{Refine hypotheses} based on further evidence and data; \emph{Construct safe care pathway}.  These are not discrete activities, nor do they occur in sequence.  Instead, doctors tend to work recursively; moving back and forth from one set of activities to another, and using the outcomes of one set to inform and guide another.

The guiding principle behind each of these sets of activities is to mitigate risk and maintain patient safety;  this is made clear not only by the order of hypotheses and steps taken, but also by the self-reflection during the think-aloud process. The highest-risk, usually life-threatening, possible explanations for a set of presented symptoms were always considered first and systematically eliminated by gathering or identifying supporting evidence.  This process was repeated for the next-highest risk hypotheses and so on, until either a single working hypotheses was devised or hypotheses could not be ruled out due to inadequate evidence. It was also clear that, during the evidence gathering phase, to find support or to eliminate any given hypotheses, clinicians carefully considered \emph{all} evidence they encountered to see if there were potential connections to either new hypotheses not yet considered, or previous ones already excluded.

Given the importance and consistent application of this workflow in both primary and secondary care settings, one might ask how self-logged information might be better designed to support this process. In our experiment, it was clear that the self-logged data we included in our scenarios were often irrelevant to the most severe hypotheses, and therefore were not appropriate for use in hypothesis elimination until much later in the process.  Yet, such data were always considered by the clinicians at least once, and usually quite early in the process, for the purposes of introducing new hypotheses not yet considered.  Therefore, in the context of hypothesis creation and elimination, it is clear that self-logged data have two potential roles: both in \emph{discovery} (e.g., identifying potential causes not yet considered), and \emph{refinement} (e.g., eliminating hypothesis or supporting existing hypotheses).

A third, related use for self-logged data in this workflow is its role as a communications aid, for helping patients explain their symptoms, or recent history.  The importance of clearly understanding what a patient was experiencing was emphasised several times during the experiment, with clinicians not only talking with the patient (seeing ``looking into the patient's eyes'') but also carefully studying the patient's body language to ascertain how the patient was feeling.  This process was particularly important during the first few minutes of a patient consultation, to set the stage for the subsequent risk-mitigation workflow process.

In the evidence gathering phase, when inadequate supporting information was available to definitively rule out a hypothesis, clinicians resorted to adding actions to plans to gather more evidence.  Such actions included conducting physical exams, simple diagnostics (such as taking blood pressure), to more time and resource extensive diagnostics, such as blood work, ECGs, and imaging.  These actions were often prioritised by how quickly, easily they could be done, and the potential value of the data gleaned, against the potential inconvenience, discomfort and risks posed to the patient.  It was clear that if more appropriate and trustworthy evidence were available (such as through self-logged sources), fewer such tests might be required, saving potentially not only time and resource, but also affording a more expedient diagnosis.  The potential for risk and discomfort in additional diagnostic tests also suggests that \emph{not} having to do such tests could bring about additional direct benefit to patients, as well. 

\subsection{Frameworks for Evidence: Form and Representation}

Somewhat unsurprisingly, we observed a critical relationship between the aforementioned process of risk-mitigation and the ways that clinical evidence was ordered, structured and represented. Our  secondary care specialists, in particular, expressed having to `re-arrange' information that was being presented to them in order for them to be able to effectively think about the evidence, and expressed frustration when trying to ascertain how particular evidence fit within the frameworks to which they were accustomed.

Aspects that were deemed particularly crucial was the timeline of patient events.  Clinicians mentioned mentally placing evidence (corresponding to patient events) along this timeline as they went through the scenarios. Both chronological ordering and duration between events were important for identifying potential relationships between symptoms, and establishing connections between symptoms and potential causes.   Such relationships were germane to evidence admission; that is, for determining whether a particular piece of evidence was relevant for the current set of hypothesised causes. During the process of restructuring evidence, several participants noticed that crucial information was missing from the scenario about the timeline of patient events.  This highlights an additional aspect of standardised representations, that they make it easier to identify when crucial information is missing.

Work identified within our literature review made similar observations regarding lack of standardisation in self-logging tools. In particular, Chung et al.\ \cite{chung_more_2015} observed that, while standardised forms for some data do exist (for example, glucose level and blood pressure), consumer self-tracking tools rarely use them, and, where they are used, reduce it to a factor which is not useful to clinicians (for example, physical activity points). They suggest that standardised forms have been designed to facilitate quick and accurate review of data, and the use of non-standard data representations makes it more difficult for clinicians to use self-logged data effectively.

Given the importance of the structuring and ordering in standard representations, we speculate whether digital tools might, where relevant, re-order and re-representing self-logged diary data into forms that clinicans are accustomed to.  If successful, such re-presentations could reduce the mental effort needed to transform presented evidence and facilitate reasoning, supporting external cognition~\cite{extcog} for the hypothesis refinement process.  Additionally, such representations might make it easier to spot irregularities in the evidence, such as missing data, errors or hidden causes.  Although not yet common in use even for clinical data, our research also suggests that visualisations of patient event timelines, if designed in a clinician-centric way, might facilitate temporal reasoning across related events.

\subsection{Unpacking ``Expertise''}

Our literature review revealed that clinicians are concerned with their lack of expertise in using self-logged data. Based on results from our experiment, we now have a clearer idea of what ``lack of expertise'' might actually mean.  We believe that this there are at least two related factors.

The first pertains to what was captured, in particular reasoning about self-logged information that fell \emph{outside the set of markers} normally used for diagnosis.  For example, within our experiment, when presented with the caffeine chart, clinicians admitted being unable to effectively decide whether the information was worth consideration.  One remarked that they weren't familiar with the normal caffeine level, (displayed in milligrams per day), and attempted to convert the scale to \emph{cups of coffee per day} to help them reason about it.  Had this been a standard marker used regularly for diagnosis, it is much more likely that interpretation of this measure would have presented less difficulty.

The second pertained to (statistical) data interpretation. With the heart rate chart, a number of clinicians said they were not confident in interpreting the data, but when asked for clarification, it became clear what they meant was they had difficulty interpreting it \emph{in the form that it was presented}.  Several of our participants interrogated the salient features of the heart rate chart -- comprising fluctuations, peaks and troughs of aggregate 3-per-day statistics, but could not discern easily if they were normal (uninteresting) or abnormal (possible evidence). This contrasts with the ways that outcomes of clinical diagnostic test results are  typically reported: with statistical likelihoods pre-calculated, and deviation from expected means explicitly represented.

Such observations, we believe have a number of obvious implications for the design of self-logging and Quantified Self tools; first, the plurality (and popularity) of tools for self-measurement already demonstrates wide heterogeneity in what can be measured, many of which pertain to routine activities that may not correspond to standard measures used by clinicians. We suggest that tools might consider ways to make it easier for clinicians to reason about such unfamiliar kinds of data.  For example, by comparing a person's measurements against a population (or demographic) average, useful comparisons could be directly made.  Statistical pre-validation could be performed by tools to determine whether such measurements fall within the normal range for people within the patient's demographic, and, like lab results, this summary could be made explicitly visible. Finally, continuous information may be better initially presented as aggregated data, highlighting significant events. 
% The emergence of The Quantified Self has already demonstrated that there exists heterogeneity in types of measurement, many of which will be unfamiliar to clinicians. We suggest that tools be designed to make it easier for clinicians to use unfamiliar forms of data. By comparing a person's measurements against a population average, salient features may be made more prominent. Continuous information may be better initially presented as aggregated data, highlighting significant events. This links back to the suggestions in Section~\ref{sec:riskmit}, where information presented in medical records tend to have the most significant pieces of information displayed most prominantly.  %Clinicians can then navigate to those events on the continous data

%, and would need consult a cardiologist to help interpret or validate their findings. We suggest that this is because clinicians are unfamiliar with the analytic process required to distinguish salient features from noise. While clinicians are familiar with heart rate as a measurement, they are not used to being presented with heart rate information in a continuous form. Clinicians would normally expect only significant findings to be presented, such as days on which the patient's heart rate was out of a normal range.  When clinicians order tests for patients, the results are aggregated by specialists such that clinicians may quickly look for important markers. For example, results haematinics tests (tests for vitamins, folic acid etc) are presented with the result of each individual test highlighted if out of normal range. This allows a clinician to quickly interpret the information without needing to consider the significance of each result. There are some continuous data formats that clinicians are familiar with, such as ECGs, but interpreting these forms part of their training as medical doctors.


% use the information because they weren't familiar with a normal caffeine level. We suggest that the primary reason for this was that caffeine is an atypical measurement for clinicians, in contrast to, for example, blood pressure, which is commonly measured in many clinical settings. Clinicians are unable to make meaningful use of data corresponding to unfamiliar measurements due to their lack of experience or training.

% and that it may be inefficient to use within tight time constraints; clinicians did indeed not always have the expertise to effectively use the patient-provided data. T


% predigest against a population normal - easier to use
% personlised / stratisfied medicine - compare against a people of the same demographics


% Building on this, clinicians raised concerns that the introduction of patient-provided data may add complexity to their practice, and they would need to make a judgement on whether using this information is efficient, and how much time they should spend on it. Supporting this judgement may therefore allow self-logged data to be more effectively used within clinical settings -- by visually illustrating the relevance of self-logged data to a scenario, the quality of the data, whether it compliments the clinician's training, and so on, a clinician can more quickly and reliably decide whether to use it.


% was perceived to hinder the hypothesis refinement process, and how the patient-supplied data fits within that timeline. 

% For example, in everything from standard electronic medical records to hospital admissions forms, to the newly proposed Summary of Care Record~\cite{}, information is more often ordered in a way to present allergies, risk factors and risk factors and disease markers are made prominent, allowing clinicians to identify high-risk diagnoses. We observed that clinicians had difficulty using the information they were given as it did not conform to the structure of a patient record that they were familiar with. They were unable to establish how particular parts of the scenario related to one another, and were not clear on how much was relevant to the patient's current condition. In trying to restructure the information as a patient record, they noted that crucial information was missing about the timeline of the events and how the patient-supplied data fits within that timeline. When presented as a separate information source, it is not clear how patient-supplied data fits within the patient's timeline.

% However, their judgement was also influenced by statistics and economics. Doctors are trained to recognise that common conditions are common, so will weight their consideration of worst-case scenarios against their likeliness. Furthermore, rare conditions are usually more expensive to test for, requiring more bespoke and time-consuming tests. For these reasons, doctors will not try to rule out worst-case scenarios based on their severity alone, but also on their probability and economic costs. This suggests that, while not immediately useful, self-logged data may still become useful early in the diagnostic process if it relates to a common condition or condition that is economic to test for.

% Our literature review only revealed a small aspect of this relating the importance of relevance. When data is not relevant to the patient's condition, it may be discarded by the clinicians. Building on this, we suggest it must be relevant to a working hypothesis to be considered. Data is not used opportunistically -- clinicians don't just use the data because it may fit a possible hypothesis. 

% To integrate well into current clinical practice, self-tracking will need to support the risk-averse strategy used by clinicians. Clinicians need to understand how self-tracked data is relevant to current hypotheses... Support ruling out hypotheses... By adopting principles used in presenting patient records records ... How it fits into a patient timeline... Form part of patient records... Medical records are designed for safety critical situations... Important that data is temporal


%, and medical tests being expensive, decisions must be made on an economic basis as well as risk averse.

% Still, we observed that clinicians often ignored data, at least initially, because it did not support their hypothesis of a worst-case scenario, and that physical examination and tests would be required to rule that out.

%* WITHHOLD JUDGEMENT: Clinicians were cautious to intervene (Withholding judgement; keep them alive without interfering too much)

% How do they relate to the literature. did it correspond to a particular theme? support or contrast?

% RELEVANCE - Individuals have little guidance from clinicians on what they should track, so don't necessarily know what information is relevant to their condition. Without guidance, individuals may devise their own systems of recording information which are not complementary to a clinician's training. They may present irrelevant information to a clinician, or, conversely, may fail to disclose relevant information which they thought was irrelevant. Clinicians may dismiss information which is inapplicable to the current situation.

% need for physical exam, how they walk in, talking to patient, Psych issues.


% \subsection{Design for Situational Constraints} % (support & respect clinical workflow?)

% What did we observe?
% EXPERTISE, SpECIALITY, COMPLEXITY, TIME, EFFORT 
% How do they relate to the literature. did it correspond to a particular theme? support or contrast?
% DATA LITERACY, SITUATIONAL CONSTRAINTS[TIME,OVERLOAD,EFFORT]
% How can we make QS apps better?



% heterogeneity


% - do the statistical tests
% "it goes up, but is that a siginificant increase?"
% highlight salient features, so it's easier to determine the significant features against noise

\subsection{Psychology of Motivations for Self-Logging} 

% What did we observe?

% How do they relate to the literature. did it correspond to a particular theme? support or contrast?

% SELECTIVE DISCLOSURE -  When presenting information to a clinician, the patient may not realise what information is relevant and fail to disclose it. Some patients raised privacy concerns with sharing information with health providers, raising concerns that information may be used against them. Some patients therefore concealed certain information from their health providers.
% DOCTOR-PATIENT RELATIONSHIP -  Clinicians are concerned that they do not have appropriate expertise or training to effectively use or validate self-logged data. Contributing factors include the large variety of forms of self-logged data, the lack of standards for data representations, the lack of access to appropriate electronic tools for analysis, and not being familiar with new tools for self-quantisation. Clinicians may rely on specialists to interpret the information, but they may not always be available.

% How can we make QS apps better?

% psychology why the data was logged - 
% three classes of self-logged data:
%  - patient - sometimes it will be curiosity/obsession (e.g. they have just happen to have a device), or  -- (doctor asks why are you doing this? self diagnosis. could be a panic attacks, but it could be something unrelated -- af; hypochondria)
%  - patient - standard mechanisms of tracking diagnostics illness, e.g. high blood pressure
%  - clinician - prescribed (e.g. telemonitoring)
% ->implications for data believability - 
%
% design challenges:
% - difficulty with dealing with first class

One of our most unexpected findings was that clinicians found it important to try to understand the reasons that people chose to meticulously self-log data about themselves, and attempted to use this line of reasoning to ascertain potentially hidden physical or psychological problems in patients. In particular, several of our participants considered whether the patient's act of recording data indicated the presence of psychological disorders, ranging from obsession to depression, which could explain some of the symptoms presented. 

This kind of reasoning, however, would only be relevant to the subset of self-logged data that patients voluntarily captured themselves, without any clinical reason for doing so.  Drawing from this, we propose three categories of self-logged data. The first comprises data which the patient recorded voluntarily, without being told to by a clinician, such as out of curiosity, personal goals, or obsession. This category would include, for example, data captured incidentally by an activity tracker purchased or acquired for the purpose of getting fit. The second describes data which have been recorded for the purpose of managing one or more chronic conditions~\cite{ancker_invisible_2015}.  Our literature review found studies documenting the often complex processes involved in keeping track of symptoms, medications and other aspects of having one or more chronic disease. The third describes data which has been recorded as a result of a clinicians instructions. This includes, for example, telemonitoring devices, such as implantable ECGs, as a result of a diagnosis of arrhythmia.

Both the caffeine chart and heart rate chart fell within the first category, and as such, prompted questions about why the patient felt the need to record these data, whether it was evidence that the patient had self-diagnosed, and if the presence of the data was a symptom of an underlying psychological issue, such as obsession, stress, or a co-morbid psychiatric disorder. Due to this possibility, clinicians were more sceptical of the accuracy of the data, suggesting that the patient may have been motivated to manipulate or fake the data to force a particular outcome from the consultation.

We suggest that, with the increasing prevalence of wearable technology and self-logging apps, the volume and variety of data falling in the first category will grow.  While this may prompt more questions about why these data were captured; simultaneously, however, that most of these data will be captured automatically by wearable sensors and devices will mean that the psychological reasons for doing so may become less significant.


% that a primary motivation of doing so will be that the patient has the data anyway. However, one danger we have identified is that patients may be presenting this information in the form of a self-diagnosis. For example, recording and presenting a plot of caffeine use may suggest the patient believes they are suffering effects of a caffeine related illness, such as withdrawal. For that reason, it is important that self-tracking technology warn the user that these devices should not be used for the purpose of self-diagnosis, and that only qualified clinicians should make those judgements.

% In order to make the patient's motivations for recording data clearly, tools may ask the patient why they are recording it, and if it is out of medical advice to do so, or the user's own choice. In presenting the information to a clinician, they can therefore be made aware of the patient's motivations.

%Clinicians should be aware that more people will do this (they're not obsessed)
%Patients will be bringing more data
%QS should highlight dangers of self-diagnosis (patients should not be coming with a diagnosis)
%Data as a symptom
%* GP said he would respect patient's efforts
%* will patients be dissatisfied when doctors dismiss data

\subsection{Data Believability: Veracity, Sampling and Context}

% One important remaining consideration
Clinicians were unwilling to make decisions based on self-logged data without knowing details about \emph{how} it was recorded. When contextualised within the risk mitigation workflow, it became clear why; for evidence to be able to be used to justify the elimination of a high-risk hypothesis (such as an undiagnosed, potentially fatal condition) it was imperative that this evidence was reliably obtained and interpreted correctly.  A number of factors were mentioned around this, including what instrument (device) had been used to perform the measurement, whether the device was calibrated, how the data were sampled.  In order to ascertain whether there were potential sampling issues for the blood pressure charts, for example, clinicians asked questions about the context(s) at the times of capture: where the patient was when the data were recorded, what the patients were doing, among others.  When satisfactory answers were not available, clinicians wanted to retake the readings using their own calibrated devices.

In order to for future wearable devices and smartphone apps to capture data that can be interpreted believably, they might capture information about the calibration of their sensors and associate these with the data.   More critically, we propose that there may be opportunities to solve some of the sampling concerns that arose in our study through the use of digital context capture techniques.  For example, physical activity sensing could enable annotation of heart rate readings with physical activities, while location sensing could provide physical location annotations as well.  Cryptographic approaches such as proof of work~\cite{clark2012commitcoin} could further be used to prove that such readings were, in fact, taken at a particular time to eliminate the possibility of post-hoc data fabrication.  The benefit of such techniques would be that they might substantially improve the trustworthiness of sensed data without adding additional burden to the process of capturing it.

% Device manufacturers and software engineers can often get an idea of reliability by running studies. If they do such studies, annotate data with their estimated reliability... regulation..


% quality, sampling problems, making it up, provenance (usb pill box vs diarising) - instruments will get more accurate, but data needs to be annotated with how accurate, when, how it was measured
% more information is not necessarily better - has to capture important things
% how do you make sure that you're recording clinically signification events? ie ensure no missing data

% What did we observe?

% How do they relate to the literature. did it correspond to a particular theme? support or contrast?

% QUALITY -  Many consumer devices available for self-tracking and self-logging are not clinical grade, with only a small number having been approved for medical use. Clinicians often perceive the data quality and sampling from non-clinical grade devices to be poor and perceive the devices to be unreliable. Clinicians often are unwilling to use such information out of fear of consequences. Studies have however demonstrated some devices to have good reliability, including the Fitbit wearable tracker.
% COMPLETENESS -  It has been observed that individual's who self log often do so sporadically, leaving out information which may be important. Individuals attributed this to cumbersome tools and the fact that it takes time and effort to track. It has been observed that clinicians look for detailed information to identify patterns and anomalies in data.
% REPRESENTATION -  Health providers use standardised forms for certain types of data, including blood pressure, symptom history and glucose level. However, due to the quantity and variety of self-tracking apps and devices, there is a lack of standardisation of how self-logged data is represented. Furthermore, applying standards to data can remove important parts. Exercise logs, for example, often have distinct data structures and different meanings of `activity level'. It has been observed that this heterogeneity in data representations adds difficulty interpreting self-logged data.
% INTEROPERABILITY - A lack of standardisation of how data is represented creates problems when trying to share between colleagues and other health providers. There are concerns that self-tracking tools will be poorly integrated into existing health provider tools. In particular, there are challenges regarding the integration of Web-based services -- which many self-tracking devices and apps are -- into legacy systems. Many self-tracking apps do not provide anyway to export data, or they only do so in propriety APIs, which are not intuitive for clinicians to use.
% DATA LITERACY [already mentioned] - Clinicians are concerned that they do not have appropriate expertise or training to effectively use or validate self-logged data. Contributing factors include the large variety of forms of self-logged data, the lack of standards for data representations, the lack of access to appropriate electronic tools for analysis, and not being familiar with new tools for self-quantisation. Clinicians may rely on specialists to interpret the information, but they may not always be available.

% How can we make QS apps better?

%\subsection{Representation: presentation in form of a medical record} % (representational flexibility)

% Clinicians were unwilling to make use of self-logged data without know how it had been recorded. A number of factors were mentioned around this, including what device had been used, if the device was clinical grade, the quality, sampling and reliability of the recorded data, how the data had been analysed and presented, and the methods which the patient had used. For the heart rate data, clinicians wanted to retake the readings using their own calibrated devices.

% Device manufacturers and software engineers can often get an idea of reliability by running studies. If they do such studies, annotate data with their estimated reliability... regulation..

% Make clear the provenance of the data: where has it come from? how has it been analysed and presented? Adopt a standard model for provenance.


% Ambiguity - units



% * timeline / linking with other information
%   * placed in context of patient history - what else was happening on those dates?
%   * comparing data to previous days
%   * comparison to normal (what's normal for this patient?)
%   * highlight inconsistencies / contradictions
% * however, clinicians didn't always trust the data
%   * clinicians needed validation, want to retake readings
 
% when readings are taking, evenly spaced - represent this graphically without need to stare at it for a long time
% 


%\subsection{Accommodate for Cognitive Bias}

%not possible to measure specific presence, but some of the results are preliminary evidence:
%\begin{itemize}
%\item  Confirmation bias - interpret symptoms according to background (e.g. pulmonology - looking for history of asthma)
%\item  Illusory patterns - pattern recognition in noise
%\item  Priming - presentation of caffeine data led some clinicians to immediately think it was a caffeine issue
%\item  Prior experience - referring back to previous patients
%\item  Stereotyping - students drink; engineers self-track
%\item  Interpreting according to personal belief - I thought heart rate was more variable than that
%\item  Data more trustworthy when it appears normal
%\end{itemize}

% implications for data representation - raw data, not salient faatures / summary; poor data format (not sused to that format) - explore further

\subsection{Implications for HCI}

% Based on clinical experts' opinions, the kinds of data that people are are keeping track of are still either mostly irrelevant or are of insufficient quality to be of effective use.  However, several of our experts believed that this could change, as high-quality sensors become increasingly ubiqutious.  , refinement and evidence piecing-together processes that clinicians engage in when seeing patients.  

The market for health tracking devices is currently being led by technology companies in fierce competition to sell new, technologically-advanced systems, rather than to best support patients' long-term needs. We feel that HCI, which has long examined personal health and well-being technologies, should continue to illuminate ways that such tools could better meet both patients' and clinicians' needs, through in-depth investigations, two examples of which we describe next.

The first is in resolving the disparity between kinds of data individuals think is important, and data that clinicians actually find useful for diagnosis.  We feel that there is room for extending the current understanding of why and how people self-log, such as studied by Choe et al.~\cite{Choe2014}, to explore ways that such practices might be aligned to better support the taking of clinically relevant, high-quality measures.  Well-being diary ``support systems'' of the future, might, for example, teach (and nudge) individuals to capture more clinically-relevant data, and guide them on how to make more accurate measurements. 

The second pertains to improving the clinicians' ability to explore and interrogate data brought in by patients during consultations. Although our study did not evaluate specific presentation or interaction techniques, it was clear that such particulars were important, as discussed earlier. But we also believe that further studies of clinicians' evidence-gathering and hypothesis-refinement workflows during consultations could greatly benefit the design of interfaces for performing \emph{in-situ}, patient-provided data sensemaking and discovery. %Such interfaces might, for example, provide a glancable view of all of a patient's self-kept data, along with metrics such as temporal coverage, granularity, completeness and quality, to make it easier for clinicians to pull up any in search of evidence to supporting a particular hypothesis. We intend to explore such interfaces in future work.

%  However, the challenges in designing such a system are many; to support the user privacy needs, for example, such interfaces would have to be ultimately under the patient's control, and operable by both patient and clinician.  However, they should also be highly familiar, uniform and efficient for clinicians.  

\section{Limitations}

The first, perhaps most obvious limitation was our sample size; the number of clinicians that we interviewed was small (3 GPs and 7 specialists).  Since the practice of clinicians may differ substantially depending on specialism, training, background, and where they work, and so forth, we would like to follow this study with another featuring a different set of GPs, and clinicians with different specialities, as well as ward and triage nurses. Second, our interviews with GPs were conducted individually, whereas our interviews with specialists were conducted as a group. As a group, people may be more selective in what they say because of the presence of colleagues. Because of this, our observations cannot be used to directly compare differences in approaches to diagnosis between primary and secondary care.  A third limitation pertains to the way that setting and scenarios were presented. Instead of seeing real patients, or a professional actor, scenarios were written as textual descriptions on paper.  As a result, clinicians did not have access to the patient (or an actor) or their medical file. This limits our ability to observe how self-logged data affects doctor-patient relationships. Fourth, since we presented data on a piece of paper and not on a device, we cannot infer the role of the user interface on the usefulness or introduction of barriers to use. Finally, we only showed them two different forms of self-logged data, in two representations within two different scenarios. We cannot necessarily understand the implications of the type of information, form of data representation on data usability. We wish to explore this further in follow-up work.

\section{Conclusion}

% This paper sour

% final synthesis of what paper was about: 
% * original question: barriers to QS
% * instead looked at opportunities to tame QS to workflow
%   * understanding psychology of why it was captured
%   * eliminating hypotheses (high-risk)
%   * understanding symptoms
% BUT what is being captured, representation problems, 
% not an attitude problem, just problems with the system


%it's not just what the data is, it's who it comes from {your friend, your colleague, your doctor}. is it {a teenager, a female student, a male student, a middle aged man, a middle aged woman}


This paper investigated challenges to the adoption of self-logged data in clinical practice. By conducting a review of literature surrounding the use of self-logged data, we identified potential factors pertaining to data capture, data access, clinical practice, and situational constraints.  Through our diagnostic role-play experiment with clinicians, we were able to get a deeper understanding of how such data would be used in practice, including factors that influenced whether these data were considered during the risk-mitigation decision-making process, barriers to effectively interrogating  these data during clinical visits, and factors that influenced whether clinicians viewed data as trustworthy. 
%We established that there exist a number of design opportunities to addressing these barriers. 

More specifically, we identified opportunities how self-logged data could help with various phases of the diagnostic workflow process, including \emph{communication} (of symptoms) with the patient,  \emph{discovery} of potential causes not yet considered, and \emph{refinement} of hypothesis.  We discovered that the motivation for self-logging activities was also of critical importance to the diagnostic process, by helping clinicians discover potential psychological disorders, or unspoken organic disorders. Finally, we identified reasons that clinicians distrusted self-logged data pertaining to accuracy of instruments, sampling methods, potentially missing data and patient activity or context, and identified potential ways that some of these issues might be addressed in the future. These initial findings suggest that, although there may be a significant number of design challenges remaining, the use of self-logged data may eventually significantly improve clinicians' abilities to effectively draw together evidence for clinical diagnosis. %anticipated barriers for clinical adoptions of self-logged data.

% if it was out of an obsession, curiosity, or if they had been instructed to do so by a clinician. Finally, the veracity, sampling and context of the data had implications for how it can be used. These initial findings suggest that there are design challenges and opportunities for overcoming some of the anticipated barriers for clinical adoptions of self-logged data.


% First, we observed that clinicians use information either to support or rule-out risky diseases, and that self-logged data were not always relevant to these early hypotheses. This presents challenges in how self-logged data can be design to facilitate \emph{discovery} (e.g. identifying potential causes not yet considered), and \emph{refinement} (e.g., eliminating hypothesis or supporting existing hypotheses). Second, clinicians find self-logged data more useful when it is obvious how it fits into the patient's medical history, and the clinician's workflow.  Third, a clinician is not an expert in all forms of measurement -- a normal caffeine level is not a well-known figure. We suggest that designers take this into consideration by representing information against a population normal. Fourth, the psychology of self-logging was of importance. Clinicians did not credit patients with ``taking charge of their health.''  Instead, clinicians needed to understand the motivations behind a patient recording the data, and if it was out of an obsession, curiosity, or if they had been instructed to do so by a clinician. Finally, the veracity, sampling and context of the data had implications for how it can be used. These initial findings suggest that there are design challenges and opportunities for overcoming some of the anticipated barriers for clinical adoptions of self-logged data.

\section{Acknowledgements}
This work was supported by the Web Science Institute (WSI) Stimulus Fund, \emph{SOCIAM: The Theory and Practice of Social Machines}, and the Web Science Doctoral Training Centre (DTC), University of Southampton. The WSI stimulus fund was awarded by the University of Southampton as part of the Higher Education Innovation Fund. The SOCIAM Project is funded by the UK Engineering and Physical Sciences Research Council (EPSRC) under grant number EP/J017728/2 and comprises the Universities of Oxford, Southampton, and Edinburgh. The Web Science DTC is part of the Research Councils UK Digital Economy Programme, funded by the EPSRC under grant number EP/G036926/1.

%This work was supported by the Web Science Institute (WSI) Stimulus Fund, and \emph{SOCIAM: The Theory and Practice of Social Machines}.  The SOCIAM Project is funded by the UK Engineering and Physical Sciences Research Council (EPSRC) under grant number EP/J017728/2 and comprises the Universities of Oxford, Southampton, and Edinburgh.

% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{wsi-chi-16}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
